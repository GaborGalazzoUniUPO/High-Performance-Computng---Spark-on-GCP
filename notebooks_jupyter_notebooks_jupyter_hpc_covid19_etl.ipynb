{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intorduction to PySPark on Jupyter Notebook\n",
    "\n",
    "Use <kbd>CTRL</kbd>+<kbd>⏎</kbd> to run cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the spark object is pre initialized on PySpark Jupyter Notebooks\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "ts = datetime.now()\n",
    "time = int(time.time())\n",
    "\n",
    "#used for time deltas\n",
    "minus = 2\n",
    "\n",
    "today = (datetime.today() - timedelta(days=minus)).strftime(\"%Y%m%d\")\n",
    "yesterday = (datetime.today() - timedelta(days=minus+1)).strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set your bucket name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bucket_name = \"INSERT_YOUR_BUCKET_NAME_HERE\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading: load data in the distributed file system\n",
    "In order to work properly, **Spark** needs to work wih **data** that can be **reached from each node of the cluster**, but this is not the only requirement, we must access to the file in a **file system like manner** in order to partition work, so we have to use a **Distributed Files System**.\n",
    "There are various alternatives to choose, the first one is the native for the **Hadoop Ecosystem HDFS (Hadoop Distributed File System)**, but now cloud vendors exose their own richer solution like **S3 for AWS** or, in this case, **Cloud Storage on GCP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#download data locally\n",
    "os.popen('wget -O data.csv https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-province/dpc-covid19-ita-province-%s.csv' % today).read()\n",
    "#load it into Google STorage DFS\n",
    "os.popen('gsutil mv data.csv gs://%s/dpc-covid19-ita-province-%s.csv' % (bucket_name,today)).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL example\n",
    "The first example consist in create an ETL process using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction phase\n",
    "The first phase is the **extraction** so we have to load data from OLTP sources like databases or files into a **Dataframe**.\n",
    "The **Dataframe** is a named column organized **Dataset**.\n",
    "\n",
    "*A **Dataset** is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine.\n",
    "A **DataFrame** is a Dataset organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.*\n",
    "\n",
    "The documentation about how to Load/Store file in Spark is here: [https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = spark.read \\\n",
    "                .option(\"header\",True)\\\n",
    "                .csv(\"gs://%s/dpc-covid19-ita-province-%s.csv\" % (bucket_name, today))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Visulalize the dataframe\n",
    "In Jupyter notebooks to analize the content of a dataframe we have to convert a DF to a Pandas Dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "In this phase we want to generate the **output dataframe** by selecting a subest of the orginal data and by adding new columns.\n",
    "In order to do this we have to import [*pyspark.sql functions*](https://spark.apache.org/docs/2.3.0/api/sql/index.html) a collection on functions used to manipulate columns of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "small_source_df = source_df.select(\"stato\",\"codice_regione\",\"denominazione_regione\",\"codice_provincia\",\"denominazione_provincia\",\"totale_casi\",F.lit(today).alias(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_source_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading\n",
    "We have loaded our file in spark via Dataset interface, we have manipulated it with sql.functions now we want to store our new View on the Data Wherehouse. In this case, our DW is made of **parquet files**, an **Apache Common File Format** that is used in a big data context. This format is **self-describing** and optimized to be partitioned, so multiple workers can consume the same file and work on it.\n",
    "\n",
    "Before storing our Dataframe let's look inside of it: a data frame is an **abstraction for a dataset** and **it doesn't contain data**! Da data frame **describe the spark procedure** to generate a Row List. Spark use the **parallel skeleton** approach to let the developer write the data frame that is converted into a **Task Graph (DAG)**; when we invoke DataframeFunction's like collect(), first() or toPandas() we are submitting the **DAG** to the cluster's master node that distributes tasks across workers than we wait until the master will gather all data in a single Row List."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_source_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before loading I'll want to bring the focus on the partition concept: as we will see on the bucket we can perform optimizations based on partitions, if we know that our data have a regionality behaviour we can split our data frame into smaller files that can be processed in parallel when we want to perform aggregate computation on partition fields."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_source_df.write.partitionBy(\"data\",\"stato\",\"codice_regione\",\"codice_provincia\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .parquet(\"gs://%s/dpc-covid19-ita-province\" % bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explore your bucket to see how this dataframe is stored and what is a parquet file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics and OLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from DW\n",
    "Load todays data from the parquet directory that is partitionated by *data* into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_source_df = spark.read.parquet(\"gs://%s/dpc-covid19-ita-province\" % bucket_name).where(F.col(\"data\") == today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Query\n",
    "In this cells we are computing some aggregate metrics, we can do it i two different ways:\n",
    "1. Using Spark SQL functions manipulating functionally the dataframe\n",
    "2. Using SQL by mapping the dataframe into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "df_by_region = small_source_df.groupBy(\"codice_regione\").agg(F.first(F.col(\"denominazione_regione\")).alias(\"denominazione_regione\"),\n",
    "                                                             F.first(F.col(\"data\")).alias(\"date\")\n",
    "                                                             ,F.sum(F.col(\"totale_casi\")).alias(\"totale_casi\"))\\\n",
    ".select(\"denominazione_regione\",\"totale_casi\",\"date\")\n",
    "df_by_region.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "small_source_df.createOrReplaceTempView(\"sds\")\n",
    "spark.sql(\"select first(denominazione_regione) as denominazione_regione, first(data) as date, sum(totale_casi) as totale_casi from sds group by codice_regione\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_by_region"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the cell below there is an example of complex computation and dataframe composition.\n",
    "*As you can see there are some Java code parts, that's because PySpark is only a wrapper for the java Spark/Hadoop below system *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a reference to the Hadoop File System\n",
    "sc = spark.sparkContext\n",
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "    sc._jvm.java.net.URI.create(\"gs://%s\" % bucket_name),\n",
    "    sc._jsc.hadoopConfiguration(),\n",
    ")\n",
    "# check if old data is present (at the first run may not exists)\n",
    "if fs.exists(sc._jvm.org.apache.hadoop.fs.Path(\"dpc-covid19-ita-region\")):\n",
    "    # extract old data from parquet\n",
    "    yesterday_data = spark.read.parquet(\"gs://%s/dpc-covid19-ita-region\" % bucket_name).where(F.col(\"date\") == yesterday)\n",
    "    # print data\n",
    "    yesterday_data.show()\n",
    "    # join new data and new data\n",
    "    joined = df_time.join(yesterday_data, yesterday_data.denominazione_regione == df_time.denominazione_regione,how='left')\n",
    "    # print joined data\n",
    "    joined.show()\n",
    "    # compute differences into 'delta' column\n",
    "    region_recap_df = joined\\\n",
    "        .withColumn(\"delta\",df_time.totale_casi - F.when(yesterday_data.totale_casi.isNull(),0).otherwise(yesterday_data.totale_casi))\\\n",
    "        .select(df_time.denominazione_regione, df_time.totale_casi, \"delta\", df_time.date)\n",
    "    pass\n",
    "else:\n",
    "    region_recap_df = df_time.withColumn(\"delta\",F.lit(0))\n",
    "region_recap_df = region_recap_df.orderBy(F.desc(\"delta\"),F.desc(\"totale_casi\"))\n",
    "# explore Dataframe\n",
    "region_recap_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save result for future usage and analytics\n",
    "Now we want to create a new table in order to keep tracking of increments. Like the example above we will store the data into a Parquet table."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# materialize this view for future analitycs\n",
    "region_recap_df.write.partitionBy(\"date\").mode(\"append\").option(\"mergeSchema\", \"true\").parquet(\"gs://%s/dpc-covid19-ita-region\" % bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want also to serve our data like an API by creating a json representing today's increments. You may have read in above documentation links that we can use the json write primitive of spark. I suggest you to try to do it in order to see what kind of data is produced.\n",
    "You will otice that this write do not produce a single json array file containing our data, but a particolar file format called [JSONLines](https://jsonlines.org/) used to parallel process a list of json object, this same format is used to store object in a document database like [MongoDB](https://www.mongodb.com/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_recap_df.write.json(\"gs://%s/dpc-covid19-ita-region-%s.json\" % (bucket_name,today))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the aggregate data are few we can generate the json directly in python and save it on the storage.\n",
    "This step is aimed at highlighting the most common use of spark as \"accelerated computing\" that is: I write a calculation procedure (the Dataframe) I send it to the server to compute in parallel on the large amount of data and obtain the recap/aggregate values (the collect()).\n",
    "Having obtained the results in the form of a list of objects, I rework them in the main language and so on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recap = region_recap_df.collect()\n",
    "json_recap = []\n",
    "for row in recap:\n",
    "    json_recap.append(row.asDict())\n",
    "import json\n",
    "with open('data.json', 'w') as outfile:\n",
    "    print(json.dumps(json_recap))\n",
    "    json.dump(json_recap, outfile)\n",
    "    outfile.close()\n",
    "    os.popen('gsutil mv data.json gs://%s/dpc-covid19-ita-region-recap.json' % bucket_name).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can write your own procedure to explore generated data, have fun!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_table = spark.read.parquet(\"gs://%s/dpc-covid19-ita-region\" % bucket_name)\n",
    "generated_table.toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}